{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "482d2575",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import torch\n",
    "# from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "# # Load tokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# num_labels = 4\n",
    "\n",
    "\n",
    "# # Load the model architecture\n",
    "# model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
    "\n",
    "# # Load the saved model state dictionary\n",
    "# model_path = \"bert_sentiment_model.pth\"\n",
    "# model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# # Set the model to evaluation mode\n",
    "# model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d987daaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.2.2 requires pyqt5<5.13, which is not installed.\n",
      "spyder 5.2.2 requires pyqtwebengine<5.13, which is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting streamlit\n",
      "  Downloading streamlit-1.34.0-py2.py3-none-any.whl (8.5 MB)\n",
      "     ---------------------------------------- 8.5/8.5 MB 783.4 kB/s eta 0:00:00\n",
      "Collecting blinker<2,>=1.0.0\n",
      "  Downloading blinker-1.8.1-py3-none-any.whl (9.5 kB)\n",
      "Collecting pydeck<1,>=0.8.0b4\n",
      "  Downloading pydeck-0.9.0-py2.py3-none-any.whl (6.9 MB)\n",
      "     ---------------------------------------- 6.9/6.9 MB 1.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in d:\\anaconda\\lib\\site-packages (from streamlit) (6.1)\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7\n",
      "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "     ------------------------------------ 207.3/207.3 kB 842.3 kB/s eta 0:00:00\n",
      "Collecting protobuf<5,>=3.20\n",
      "  Downloading protobuf-4.25.3-cp39-cp39-win_amd64.whl (413 kB)\n",
      "     ------------------------------------ 413.4/413.4 kB 920.4 kB/s eta 0:00:00\n",
      "Collecting rich<14,>=10.14.0\n",
      "  Downloading rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "     -------------------------------------- 240.7/240.7 kB 1.8 MB/s eta 0:00:00\n",
      "Collecting altair<6,>=4.0\n",
      "  Downloading altair-5.3.0-py3-none-any.whl (857 kB)\n",
      "     -------------------------------------- 857.8/857.8 kB 1.3 MB/s eta 0:00:00\n",
      "Collecting pyarrow>=7.0\n",
      "  Downloading pyarrow-16.0.0-cp39-cp39-win_amd64.whl (25.9 MB)\n",
      "     -------------------------------------- 25.9/25.9 MB 677.2 kB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy<2,>=1.19.3 in d:\\anaconda\\lib\\site-packages (from streamlit) (1.21.5)\n",
      "Requirement already satisfied: pandas<3,>=1.3.0 in d:\\anaconda\\lib\\site-packages (from streamlit) (1.4.4)\n",
      "Requirement already satisfied: click<9,>=7.0 in d:\\anaconda\\lib\\site-packages (from streamlit) (8.0.4)\n",
      "Collecting cachetools<6,>=4.0\n",
      "  Downloading cachetools-5.3.3-py3-none-any.whl (9.3 kB)\n",
      "Collecting tenacity<9,>=8.1.0\n",
      "  Downloading tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in d:\\anaconda\\lib\\site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3.0 in d:\\anaconda\\lib\\site-packages (from streamlit) (4.11.0)\n",
      "Requirement already satisfied: packaging<25,>=16.8 in d:\\anaconda\\lib\\site-packages (from streamlit) (21.3)\n",
      "Requirement already satisfied: requests<3,>=2.27 in d:\\anaconda\\lib\\site-packages (from streamlit) (2.28.1)\n",
      "Requirement already satisfied: watchdog>=2.1.5 in d:\\anaconda\\lib\\site-packages (from streamlit) (2.1.6)\n",
      "Requirement already satisfied: pillow<11,>=7.1.0 in d:\\anaconda\\lib\\site-packages (from streamlit) (9.2.0)\n",
      "Requirement already satisfied: jsonschema>=3.0 in d:\\anaconda\\lib\\site-packages (from altair<6,>=4.0->streamlit) (4.16.0)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda\\lib\\site-packages (from altair<6,>=4.0->streamlit) (2.11.3)\n",
      "Requirement already satisfied: toolz in d:\\anaconda\\lib\\site-packages (from altair<6,>=4.0->streamlit) (0.11.2)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\lib\\site-packages (from click<9,>=7.0->streamlit) (0.4.5)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "     ---------------------------------------- 62.7/62.7 kB 1.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in d:\\anaconda\\lib\\site-packages (from packaging<25,>=16.8->streamlit) (3.0.9)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\anaconda\\lib\\site-packages (from pandas<3,>=1.3.0->streamlit) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in d:\\anaconda\\lib\\site-packages (from pandas<3,>=1.3.0->streamlit) (2.8.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests<3,>=2.27->streamlit) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in d:\\anaconda\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2.0.4)\n",
      "Collecting pygments<3.0.0,>=2.13.0\n",
      "  Downloading pygments-2.17.2-py3-none-any.whl (1.2 MB)\n",
      "     ---------------------------------------- 1.2/1.2 MB 913.0 kB/s eta 0:00:00\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "     -------------------------------------- 87.5/87.5 kB 978.3 kB/s eta 0:00:00\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in d:\\anaconda\\lib\\site-packages (from jinja2->altair<6,>=4.0->streamlit) (2.0.1)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in d:\\anaconda\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.18.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in d:\\anaconda\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (21.4.0)\n",
      "Collecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Requirement already satisfied: six>=1.5 in d:\\anaconda\\lib\\site-packages (from python-dateutil>=2.8.1->pandas<3,>=1.3.0->streamlit) (1.16.0)\n",
      "Installing collected packages: tenacity, smmap, pygments, pyarrow, protobuf, mdurl, cachetools, blinker, pydeck, markdown-it-py, gitdb, rich, gitpython, altair, streamlit\n",
      "  Attempting uninstall: tenacity\n",
      "    Found existing installation: tenacity 8.0.1\n",
      "    Uninstalling tenacity-8.0.1:\n",
      "      Successfully uninstalled tenacity-8.0.1\n",
      "  Attempting uninstall: pygments\n",
      "    Found existing installation: Pygments 2.11.2\n",
      "    Uninstalling Pygments-2.11.2:\n",
      "      Successfully uninstalled Pygments-2.11.2\n",
      "Successfully installed altair-5.3.0 blinker-1.8.1 cachetools-5.3.3 gitdb-4.0.11 gitpython-3.1.43 markdown-it-py-3.0.0 mdurl-0.1.2 protobuf-4.25.3 pyarrow-16.0.0 pydeck-0.9.0 pygments-2.17.2 rich-13.7.1 smmap-5.0.1 streamlit-1.34.0 tenacity-8.2.3\n"
     ]
    }
   ],
   "source": [
    "!pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2d0a134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wikipedia-apiNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.org', port=443): Read timed out. (read timeout=15)\")': /simple/wikipedia-api/\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rpcio (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading Wikipedia_API-0.6.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\anaconda3\\lib\\site-packages (from wikipedia-api) (2.27.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->wikipedia-api) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->wikipedia-api) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->wikipedia-api) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->wikipedia-api) (3.3)\n",
      "Installing collected packages: wikipedia-api\n",
      "Successfully installed wikipedia-api-0.6.0\n"
     ]
    }
   ],
   "source": [
    "pip install wikipedia-api\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "199e1382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing interface.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile interface.py\n",
    "import streamlit as st\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz\n",
    "import requests\n",
    "from bs4 import BeautifulSoup  # Import BeautifulSoup for HTML parsing\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "num_labels = 4\n",
    "\n",
    "# Load the model architecture\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n",
    "\n",
    "# Load the saved model state dictionary\n",
    "model_path = \"bert_sentiment_model.pth\"\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Define labels and label encoder\n",
    "labels = [\"fraud\", \"drug\", \"kidnap\", \"murder\"]\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(labels)\n",
    "\n",
    "# Load your judge data\n",
    "judge = pd.read_csv(\"LatJudge.csv\")\n",
    "\n",
    "# Define a function to predict labels\n",
    "def predict_label(model, tokenizer, text):\n",
    "    # Tokenize the input text\n",
    "    encoded_text = tokenizer.encode_plus(\n",
    "        text,\n",
    "        max_length=128,  # Adjust as needed based on your model's input size\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    # Move input tensors to the appropriate device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    input_ids = encoded_text['input_ids'].to(device)\n",
    "    attention_mask = encoded_text['attention_mask'].to(device)\n",
    "\n",
    "    # Make predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # Convert logits to predicted labels\n",
    "    _, predicted_label = torch.max(logits, 1)\n",
    "\n",
    "    # Decode predicted label using label encoder\n",
    "    predicted_label = predicted_label.item()\n",
    "    predicted_label_text = label_encoder.inverse_transform([predicted_label])[0]\n",
    "\n",
    "    return predicted_label_text\n",
    "\n",
    "# Function to filter judge data and calculate similarity scores\n",
    "def process_data_and_similarity(input_text, predicted_label_text, judge_data):\n",
    "    # Filter the judge data based on predicted label\n",
    "    filtered_df = judge_data[judge_data['Name'].str.lower().str.contains(predicted_label_text.lower())]\n",
    "\n",
    "    # Calculate similarity scores using fuzzywuzzy\n",
    "    def get_similarity_score(row):\n",
    "        return fuzz.token_sort_ratio(input_text.lower(), row['Name'].lower())\n",
    "\n",
    "    # Apply the function to the DataFrame to get similarity scores\n",
    "    filtered_df['Similarity'] = filtered_df.apply(get_similarity_score, axis=1)\n",
    "\n",
    "    # Get top 5 most similar instances\n",
    "    top_similar = filtered_df.nlargest(5, 'Similarity')\n",
    "\n",
    "    return filtered_df, top_similar\n",
    "\n",
    "# Function to fetch information from Wikipedia and clean HTML tags\n",
    "def fetch_wikipedia_info(question):\n",
    "    # Define custom headers with user agent\n",
    "    headers = {\n",
    "        'User-Agent': 'Your-User-Agent-Name/1.0'  # Replace 'Your-User-Agent-Name' with your actual user agent\n",
    "    }\n",
    "\n",
    "    # Make a request to the Wikipedia API with custom headers\n",
    "    url = f\"https://en.wikipedia.org/w/api.php?action=query&prop=extracts&format=json&exintro=&titles={question}\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    # Check if the request was successful and process the response\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        pages = data['query']['pages']\n",
    "        page_id = list(pages.keys())[0]\n",
    "        page_data = pages[page_id]\n",
    "        if 'extract' in page_data:\n",
    "            # Parse HTML content using BeautifulSoup to remove tags\n",
    "            soup = BeautifulSoup(page_data['extract'], 'html.parser')\n",
    "            return soup.get_text()\n",
    "        else:\n",
    "            return \"No information found on Wikipedia.\"\n",
    "    else:\n",
    "        return \"Error fetching data from Wikipedia.\"\n",
    "\n",
    "# Streamlit UI\n",
    "st.title(\"Crime Category Prediction\")\n",
    "\n",
    "# Input text box for user input\n",
    "user_input = st.text_input(\"Enter a sentence describing a crime:\")\n",
    "\n",
    "# Button to trigger prediction and processing\n",
    "if st.button(\"Predict and Process\"):\n",
    "    if user_input.strip() == \"\":\n",
    "        st.error(\"Please enter a sentence.\")\n",
    "    else:\n",
    "        # Predict the label\n",
    "        predicted_label = predict_label(model, tokenizer, user_input)\n",
    "        st.success(f\"Predicted Label: {predicted_label}\")\n",
    "\n",
    "        # Process data and calculate similarity\n",
    "        filtered_data, top_similar_instances = process_data_and_similarity(user_input, predicted_label, judge)\n",
    "\n",
    "        # Display the filtered data and top similar instances\n",
    "        st.write(\"Filtered Data:\")\n",
    "        st.write(filtered_data)\n",
    "\n",
    "        st.write(\"Top 5 most similar instances:\")\n",
    "        st.write(top_similar_instances)\n",
    "\n",
    "# Input box for Wikipedia question\n",
    "wiki_question = st.text_input(\"Ask a question to fetch information from Wikipedia:\")\n",
    "\n",
    "# Button to trigger Wikipedia information fetching\n",
    "if st.button(\"Fetch Wikipedia Info\"):\n",
    "    if wiki_question.strip() == \"\":\n",
    "        st.error(\"Please enter a question.\")\n",
    "    else:\n",
    "        # Fetch information from Wikipedia\n",
    "        wiki_info = fetch_wikipedia_info(wiki_question)\n",
    "        st.write(\"Wikipedia Information:\")\n",
    "        st.write(wiki_info)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
